{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_summarization_using_sumy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPWxB+7MVwiPg3hqGLbrkky"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"HyFSzA4AcAcL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1599362619698,"user_tz":-330,"elapsed":19189,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"d608c689-f563-488f-c3d3-af6406395055"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6cKLuXlRcN_u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"status":"ok","timestamp":1599363626893,"user_tz":-330,"elapsed":12465,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"3f3099c6-1ce8-4180-c0bb-83f9547ae0a7"},"source":["# install sumy\n","!pip install sumy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sumy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n","\r\u001b[K     |████                            | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 2.4MB/s \n","\u001b[?25hCollecting breadability>=0.1.20\n","  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy) (3.2.5)\n","Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy) (0.6.2)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy) (2.23.0)\n","Collecting pycountry>=18.2.23\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 8.2MB/s \n","\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2.10)\n","Building wheels for collected packages: breadability, pycountry\n","  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21681 sha256=98fda96587e57f83b18e490ec10400aca6dc0fcb213878f173b3a6f770795482\n","  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n","  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746865 sha256=d97818a8e60d36f6c96852de0f180f9b738e26b147b182d389bb26a34608cc35\n","  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n","Successfully built breadability pycountry\n","Installing collected packages: breadability, pycountry, sumy\n","Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SS4gStE_gFFP","colab_type":"code","colab":{}},"source":["import sumy\n","from sumy.parsers.plaintext import PlaintextParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.lex_rank import LexRankSummarizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sfbK4LCOg3LH","colab_type":"code","colab":{}},"source":["document1 = \"The bag of words method is simple to understand and easy to implement. This method is mostly used in language modeling and text classification tasks. The concept behind this method is straightforward. In this method, we will represent sentences into vectors with the frequency of words that are occurring in those sentences. The process of dividing each sentence into words or smaller parts. Here each word or symbol is called a token. After tokenization we will take unique words from the corpus. Here corpus means the tokens we have from all the documents we are considering for the bag of words creation. In Bag of word representation we have more zeros in the sparse matrices. The size of the matrix  will be increased based on the total number of words in the corpus. In real world applications corpus will contain thousands of words. So we need more resources to build analytics models with  this type of technique for large datasets. This drawback will be overcome in the next word embedding techniques. Now let’s learn how to implement the bag of words technique in python with Sklearn.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOsmvFw5hXbk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":137},"executionInfo":{"status":"ok","timestamp":1599363989698,"user_tz":-330,"elapsed":627,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"0e3440a7-dac7-43c8-c8b5-0c21980e684b"},"source":["document1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The bag of words method is simple to understand and easy to implement. This method is mostly used in language modeling and text classification tasks. The concept behind this method is straightforward. In this method, we will represent sentences into vectors with the frequency of words that are occurring in those sentences. The process of dividing each sentence into words or smaller parts. Here each word or symbol is called a token. After tokenization we will take unique words from the corpus. Here corpus means the tokens we have from all the documents we are considering for the bag of words creation. In Bag of word representation we have more zeros in the sparse matrices. The size of the matrix  will be increased based on the total number of words in the corpus. In real world applications corpus will contain thousands of words. So we need more resources to build analytics models with  this type of technique for large datasets. This drawback will be overcome in the next word embedding techniques. Now let’s learn how to implement the bag of words technique in python with Sklearn.'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"AKxmM-NahsZ6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1599364073185,"user_tz":-330,"elapsed":2011,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"1ff88c05-34df-4cf4-af0d-7c91a317e965"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"NiSkiq0ShmL7","colab_type":"code","colab":{}},"source":["# initialize parser , we can inintialize parser with from_string & from_file\n","parser = PlaintextParser.from_string(document1, Tokenizer('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f-nKcBhQdR-K","colab_type":"text"},"source":["### Here 3 types of Summrizer methods are available in sumy\n","\n","*   LexRankSummarizer\n","*   LuhnSummarizer\n","*   LsaSummarizer  "]},{"cell_type":"markdown","metadata":{"id":"EXjxuG8neLlr","colab_type":"text"},"source":["##### LexRankSummarizer summarizer method"]},{"cell_type":"code","metadata":{"id":"MGgioNjiiL9Y","colab_type":"code","colab":{}},"source":["summarizer = LexRankSummarizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLnT6PndiRR5","colab_type":"code","colab":{}},"source":["summary = summarizer(parser.document, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLLRx7d1icJs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599364327472,"user_tz":-330,"elapsed":1165,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"7002e9f1-5573-4a8b-eef1-100cf8ea428d"},"source":["for sent in summary:\n","  print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The bag of words method is simple to understand and easy to implement.\n","In this method, we will represent sentences into vectors with the frequency of words that are occurring in those sentences.\n","Here corpus means the tokens we have from all the documents we are considering for the bag of words creation.\n","Now let’s learn how to implement the bag of words technique in python with Sklearn.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dmHJgmcaeTCk","colab_type":"text"},"source":["##### LuhnSummarizer summarizer method"]},{"cell_type":"code","metadata":{"id":"JCrv_Eggi5H0","colab_type":"code","colab":{}},"source":["from sumy.summarizers.luhn import LuhnSummarizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qF-aa7gvjJVS","colab_type":"code","colab":{}},"source":["summarizer_1 = LuhnSummarizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGNG30zcjMZr","colab_type":"code","colab":{}},"source":["summary1 = summarizer_1(parser.document, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4E_dlSrBjTZr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599364494121,"user_tz":-330,"elapsed":970,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"1b3b10b1-36f4-41c6-ecc5-a78df32c4acf"},"source":["for sent in summary1:\n","  print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The bag of words method is simple to understand and easy to implement.\n","In this method, we will represent sentences into vectors with the frequency of words that are occurring in those sentences.\n","Here corpus means the tokens we have from all the documents we are considering for the bag of words creation.\n","Now let’s learn how to implement the bag of words technique in python with Sklearn.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y_10WGQSeXdQ","colab_type":"text"},"source":["##### LsaSummarizer summarizer method"]},{"cell_type":"code","metadata":{"id":"eNIsj9YAj8RY","colab_type":"code","colab":{}},"source":["from sumy.summarizers.lsa import LsaSummarizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlYGb4AJkCVm","colab_type":"code","colab":{}},"source":["summarizer_2 = LsaSummarizer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1F159M9kGnQ","colab_type":"code","colab":{}},"source":["summary_2 = summarizer_2(parser.document, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccgU_5AokSQ7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599364920238,"user_tz":-330,"elapsed":727,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"7b197b7f-fcdc-43df-c901-46fed885d75e"},"source":["for sent in summary_2:\n","  print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This method is mostly used in language modeling and text classification tasks.\n","So we need more resources to build analytics models with  this type of technique for large datasets.\n","This drawback will be overcome in the next word embedding techniques.\n","Now let’s learn how to implement the bag of words technique in python with Sklearn.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"du491U7Eeb8b","colab_type":"text"},"source":["#### we can also apply different kinds of preprocessing techniques on text such as stop words, stemmer, etc."]},{"cell_type":"code","metadata":{"id":"BWjbNgEzmNF0","colab_type":"code","colab":{}},"source":["from sumy.nlp.stemmers import Stemmer\n","from sumy.utils import get_stop_words\n","summarizer_3 = LsaSummarizer()\n","summarizer_3 = LsaSummarizer(stemmer= Stemmer('english'))\n","summarizer_3.stop_words = get_stop_words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lsit-ztqogvL","colab_type":"code","colab":{}},"source":["summary_3 = summarizer_3(parser.document, 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b43S1KJNonff","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1599365877874,"user_tz":-330,"elapsed":1051,"user":{"displayName":"sharmila projects","photoUrl":"","userId":"06535329016763322144"}},"outputId":"5685ae0e-7b95-48d5-ec2f-f0d87d67d6c6"},"source":["for sent in summary_3:\n","  print(sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The bag of words method is simple to understand and easy to implement.\n","The size of the matrix  will be increased based on the total number of words in the corpus.\n","So we need more resources to build analytics models with  this type of technique for large datasets.\n","Now let’s learn how to implement the bag of words technique in python with Sklearn.\n"],"name":"stdout"}]}]}